# Задание 2: Анализ архитектур CNN
## 2.1. Влияние размера ядра свертки

#### 1. Создание моделей.
* Взял модель CIFARCNN из репозитория домашнего задания. Добавил возможность устанавливать размер ядер свертки.
* Создалл модель ComboCIFARCNN для 1х1 + 3х3 размера ядер.
#### 2. Сравнил точность и время обучения.
- Валидационная точность.

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/val_accuracy_task_2_1.jpg)

Вывод по графику:
* Наилучшая точность у 1x1 + 3x3: Модель с комбинацией ядер 1x1 и 3x3 (красная линия) демонстрирует наивысшую точность, достигая около 50% к 14-й итерации, с постепенным ростом на протяжении всего периода.
* Умеренная точность (относительно 1х1 + 3х3) у 5x5.
* Плохая производительность у 3x3 и 7x7.

- Время обучения:

  ![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/train_time_task_2_1.jpg)

Выводы по графику:
* Наименьшее время обучения у модели 3x3.
* Чуть больше потрачено времени на обучение у моделей 5x5 и 7x7.
* Долго же всех обучалась модель 1х1 + 3х3.

#### 3. Анализ размера рецептивного слоя.

- График анализа размера рецептивного слоя:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/comparison_results_task_2_1.jpg)

Выводы по графику:
* Наибольший размер рецептивного поля у 7x7: Ядро 7x7 (зеленый столбец) имеет наибольший размер рецептивного поля, достигая примерно 20 пикселей.
* Средний размер у 5x5: Ядро 5x5 показывает размер рецептивного поля около 15 пикселей, что меньше, чем у 7x7, но больше, чем у других конфигураций.
* Меньший размер у 3x3 и 1x1 + 3x3: Ядро 3x3 имеет размер рецептивного поля около 10 пикселей, а комбинация 1x1 + 3x3 — также около 10 пикселей.
* Общий вывод: Большие ядра (7x7) обеспечивают наибольший размер рецептивного поля, что может быть полезно для захвата более глобальных признаков. Однако комбинация меньших ядер (1x1 + 3x3) достигает сопоставимого размера с 3x3, что может быть более эффективным с точки зрения вычислений и гибкости модели.

#### 4. Визуализация активации первого слоя у каждой модели:

- График для ядер 3х3:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/3x3_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие активаций: Активации показывают разнообразные паттерны, что указывает на то, что первый сверточный слой извлекает различные низкоуровневые признаки из входных данных.
* Интенсивность активаций: Цветовая карта демонстрирует разную интенсивность активаций. Синие области указывают на низкие значения, тогда как желтые — на высокие. Это говорит о том, что некоторые регионы изображения активируют нейроны сильнее, чем другие.
* Пространственная структура: Многие активационные карты имеют четкие структуры, что может свидетельствовать о том, что ядро 3x3 эффективно выделяет локальные особенности входных данных.
* Равномерность распределения: Активации распределены относительно равномерно по всем картам, что может указывать на хорошую инициализацию весов и отсутствие явных проблем, таких как "мертвые" нейроны.

- График для ядер 5х5:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/5x5_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие паттернов: Активации демонстрируют разнообразные структуры, что указывает на то, что ядро 5x5 извлекает более крупные и сложные низкоуровневые признаки по сравнению с меньшими ядрами, такими как 3x3.
* Интенсивность активаций: Цветовая карта показывает разную интенсивность. Синие области соответствуют низким значениям, а желтые — высоким. Некоторые карты имеют яркие желтые участки, что говорит о сильной активации в определенных регионах.
* Пространственная покрытие: Из-за большего размера ядра (5x5) активационные карты охватывают более широкие области входных данных, что может указывать на более глобальное восприятие признаков по сравнению с 3x3.
* Равномерность и вариативность: Активации распределены относительно равномерно, но некоторые карты показывают более выраженные паттерны, что свидетельствует о том, что модель выделяет специфические особенности данных.

- График для ядер 7х7:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/7x7_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие паттернов: Активационные карты показывают разнообразные структуры, что указывает на то, что ядро 7x7 извлекает еще более крупные и сложные низкоуровневые признаки по сравнению с ядрами 3x3 и 5x5;
* Интенсивность активаций;
* Некоторая неравномерность: Некоторые карты имеют яркие желтые или темные синие области, что свидетельствует о том, что модель выделяет специфические признаки, но распределение активаций менее равномерное по сравнению с меньшими ядрами, что может указывать на потенциальную чувствительность к шуму или менее точную локализацию.

- График для ядер 1х1 + 3х3:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/1x1%20%2B%203x3_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие паттернов;
* Интенсивность активаций;
* Сбалансированное покрытие: Комбинация 1x1 и 3x3 позволяет модели охватывать как мелкие детали (благодаря 1x1), так и немного более широкие области (благодаря 3x3), что отражается в разнообразии активационных карт.
* Равномерность и выраженность.

