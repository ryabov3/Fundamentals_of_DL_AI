# Задание 2: Анализ архитектур CNN
## 2.1. Влияние размера ядра свертки

#### 1. Создание моделей.
* Взял модель CIFARCNN из репозитория домашнего задания. Добавил возможность устанавливать размер ядер свертки.
* Создалл модель ComboCIFARCNN для 1х1 + 3х3 размера ядер.
#### 2. Сравнил точность и время обучения.
- Валидационная точность.

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/val_accuracy_task_2_1.jpg)

Вывод по графику:
* Наилучшая точность у 1x1 + 3x3: Модель с комбинацией ядер 1x1 и 3x3 (красная линия) демонстрирует наивысшую точность, достигая около 50% к 14-й итерации, с постепенным ростом на протяжении всего периода.
* Умеренная точность (относительно 1х1 + 3х3) у 5x5.
* Плохая производительность у 3x3 и 7x7.

- Время обучения:

  ![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/train_time_task_2_1.jpg)

Выводы по графику:
* Наименьшее время обучения у модели 3x3.
* Чуть больше потрачено времени на обучение у моделей 5x5 и 7x7.
* Долго же всех обучалась модель 1х1 + 3х3.

#### 3. Анализ размера рецептивного слоя.

- График анализа размера рецептивного слоя:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/comparison_results_task_2_1.jpg)

Выводы по графику:
* Наибольший размер рецептивного поля у 7x7: Ядро 7x7 (зеленый столбец) имеет наибольший размер рецептивного поля, достигая примерно 20 пикселей.
* Средний размер у 5x5: Ядро 5x5 показывает размер рецептивного поля около 15 пикселей, что меньше, чем у 7x7, но больше, чем у других конфигураций.
* Меньший размер у 3x3 и 1x1 + 3x3: Ядро 3x3 имеет размер рецептивного поля около 10 пикселей, а комбинация 1x1 + 3x3 — также около 10 пикселей.
* Общий вывод: Большие ядра (7x7) обеспечивают наибольший размер рецептивного поля, что может быть полезно для захвата более глобальных признаков. Однако комбинация меньших ядер (1x1 + 3x3) достигает сопоставимого размера с 3x3, что может быть более эффективным с точки зрения вычислений и гибкости модели.

#### 4. Визуализация активации первого слоя у каждой модели:

- График для ядер 3х3:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/3x3_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие активаций: Активации показывают разнообразные паттерны, что указывает на то, что первый сверточный слой извлекает различные низкоуровневые признаки из входных данных.
* Интенсивность активаций: Цветовая карта демонстрирует разную интенсивность активаций. Синие области указывают на низкие значения, тогда как желтые — на высокие. Это говорит о том, что некоторые регионы изображения активируют нейроны сильнее, чем другие.
* Пространственная структура: Многие активационные карты имеют четкие структуры, что может свидетельствовать о том, что ядро 3x3 эффективно выделяет локальные особенности входных данных.
* Равномерность распределения: Активации распределены относительно равномерно по всем картам, что может указывать на хорошую инициализацию весов и отсутствие явных проблем, таких как "мертвые" нейроны.

- График для ядер 5х5:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/5x5_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие паттернов: Активации демонстрируют разнообразные структуры, что указывает на то, что ядро 5x5 извлекает более крупные и сложные низкоуровневые признаки по сравнению с меньшими ядрами, такими как 3x3.
* Интенсивность активаций: Цветовая карта показывает разную интенсивность. Синие области соответствуют низким значениям, а желтые — высоким. Некоторые карты имеют яркие желтые участки, что говорит о сильной активации в определенных регионах.
* Пространственная покрытие: Из-за большего размера ядра (5x5) активационные карты охватывают более широкие области входных данных, что может указывать на более глобальное восприятие признаков по сравнению с 3x3.
* Равномерность и вариативность: Активации распределены относительно равномерно, но некоторые карты показывают более выраженные паттерны, что свидетельствует о том, что модель выделяет специфические особенности данных.

- График для ядер 7х7:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/7x7_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие паттернов: Активационные карты показывают разнообразные структуры, что указывает на то, что ядро 7x7 извлекает еще более крупные и сложные низкоуровневые признаки по сравнению с ядрами 3x3 и 5x5;
* Интенсивность активаций;
* Некоторая неравномерность: Некоторые карты имеют яркие желтые или темные синие области, что свидетельствует о том, что модель выделяет специфические признаки, но распределение активаций менее равномерное по сравнению с меньшими ядрами, что может указывать на потенциальную чувствительность к шуму или менее точную локализацию.

- График для ядер 1х1 + 3х3:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/1x1%20%2B%203x3_activations_task_2_1.jpg)

Выводы по графику:
* Разнообразие паттернов;
* Интенсивность активаций;
* Сбалансированное покрытие: Комбинация 1x1 и 3x3 позволяет модели охватывать как мелкие детали (благодаря 1x1), так и немного более широкие области (благодаря 3x3), что отражается в разнообразии активационных карт.
* Равномерность и выраженность.

## 2.2. Влияние глубины CNN.

#### 1. Создание моделей.
* Создал несколько моделей разной глубины.
#### 2. Сравнение точности и времени обучения.

- Время обучения и точность каждой модели:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/train_time_task_2_2.jpg)

Выводы по скриншоту:
* Быстрее всех обучилась модель с 2 слоями. Она показала самую низкую точность (73.47%).
* Медленнее обучилась модель с 4 слоями. Обладает большей точностью, чем с 2 слоями (76.79%).
* Самые медленные модели: Модель с Residual блоками, обладающая самой высокой точностью (81.06%) и Модель с 6 слоями, обладающая большей точностью (78.4), чем модели с 2 и 4 слоями, но меньшей чем модель с Residual блоками.

- График точность каждой модели по эпохам:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/accuracy_comparision_diff_layers_cnn_task_2_2.jpg)

#### 3. Анализ градиентов для разных моделей.

- График средней нормы градиентов по эпохам:

![Image alt](https://github.com/ryabov3/Fundamentals_of_DL_AI/blob/main/%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%204/plots/grads_comparasion_diff_layers_cnn_task_2_2.jpg)

Выводы по графику:

* Модель Small (2 слоя):
** Норма градиентов (синяя линия) начинает с низкого значения, резко возрастает к 2-й эпохе, затем плавно увеличивается до 3-й эпохи и стабилизируется на уровне около 10^(-1) к 8-й эпохе.
** Указывает на быструю адаптацию градиентов в начале обучения, но ограниченную глубину модели.
* Модель Medium (4 слоя):
** Норма градиентов показывает постепенный рост с 1-й до 5-й эпохи, достигая пика около 10^(-1), после чего слегка снижается и стабилизируется.
** Отмечается более стабильное поведение по сравнению с Small, что связано с увеличением глубины.
* Модель Deep (6 слоев):
** Норма градиентов начинается с низкого значения, постепенно растет до 6-й эпохи, достигая уровня около 10^(-1), и затем стабилизируется.
** Рост медленнее, чем у Small и Medium, что может указывать на проблемы с потоком градиентов в более глубокой сети без Residual блоков.
* Модель CNNWithResidual:
** Норма градиентов демонстрирует нестабильное поведение: низкая на начальных эпохах, затем резкий рост к 7-й эпохе с пиком выше 10^(-1), после чего слегка снижается.
** Высокая норма градиентов к концу обучения указывает на эффективный поток градиентов благодаря Residual блокам, что позволяет модели лучше обучаться.
* Общий вывод:
** Модели с меньшей глубиной (Small, Medium) быстрее адаптируют градиенты, но их норма стабилизируется на более низком уровне.
** Deep модель показывает медленный рост нормы, что может сигнализировать о проблемах с исчезающими градиентами в глубокой сети без Residual блоков.
** CNNWithResidual демонстрирует наибольший рост нормы градиентов к поздним эпохам, что объясняет её эффективность за счет улучшенного потока градиентов через остаточные соединения, позволяя модели лучше обучаться и достигать высокой точности.

